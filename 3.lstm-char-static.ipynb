{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12194"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('entities-bm.txt','r') as fopen:\n",
    "    texts= list(filter(None, fopen.read().split('\\n')))\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx = {'PAD': 0}\n",
    "char2idx = {'PAD': 0}\n",
    "tag_idx = 1\n",
    "char_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = texts\n",
    "# dataset is too small\n",
    "test_texts = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(word, lower=True):\n",
    "    if lower:\n",
    "        word = word.lower()\n",
    "    else:\n",
    "        if word.isupper():\n",
    "            word = word.title()\n",
    "    word = re.sub('[^A-Za-z0-9\\- ]+', '', word)\n",
    "    if word.isdigit():\n",
    "        word = 'NUM'\n",
    "    return word\n",
    "\n",
    "def read_file(f):\n",
    "    global tag_idx, char_idx\n",
    "    words, tags, X, Y = [], [], [], []\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if (len(line) == 0 or line.startswith(\"-DOCSTART-\")):\n",
    "            continue\n",
    "        else:\n",
    "            ls = line.split(' ')\n",
    "            if len(ls) > 1:\n",
    "                word, tag = ls[0],ls[-1]\n",
    "            else:\n",
    "                word = ls[0]\n",
    "                tag = 'O'\n",
    "            word = process_word(word)\n",
    "            if len(word) < 1:\n",
    "                continue\n",
    "            char_ids = []\n",
    "            for c in word:\n",
    "                if c not in char2idx:\n",
    "                    char2idx[c] = char_idx\n",
    "                    char_idx += 1\n",
    "                char_ids.append(char2idx[c])\n",
    "            words += [word]\n",
    "            tags += [tag]\n",
    "            X.append(char_ids)\n",
    "            if tag not in tag2idx:\n",
    "                tag2idx[tag] = tag_idx\n",
    "                tag_idx += 1\n",
    "            Y.append(tag2idx[tag])\n",
    "                        \n",
    "    return words, tags, X, np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words, train_tags, train_X, train_Y = read_file(train_texts)\n",
    "test_words, test_tags, test_X, test_Y = read_file(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, size_layer, num_layers, embedded_size,\n",
    "                 dict_size, dimension_output, learning_rate):\n",
    "        \n",
    "        def cells(size, reuse=False):\n",
    "            return tf.nn.rnn_cell.LSTMCell(size,initializer=tf.orthogonal_initializer(),reuse=reuse)\n",
    "        \n",
    "        self.X = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y = tf.placeholder(tf.float32, [None, dimension_output])\n",
    "        encoder_embeddings = tf.Variable(tf.random_uniform([dict_size, embedded_size], -1, 1))\n",
    "        encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n",
    "        \n",
    "        for n in range(num_layers):\n",
    "            (out_fw, out_bw), (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = cells(size_layer // 2),\n",
    "                cell_bw = cells(size_layer // 2),\n",
    "                inputs = encoder_embedded,\n",
    "                dtype = tf.float32,\n",
    "                scope = 'bidirectional_rnn_%d'%(n))\n",
    "            encoder_embedded = tf.concat((out_fw, out_bw), 2)\n",
    "        W = tf.get_variable('w',shape=(size_layer, dimension_output),initializer=tf.orthogonal_initializer())\n",
    "        b = tf.get_variable('b',shape=(dimension_output),initializer=tf.zeros_initializer())\n",
    "        self.logits = tf.matmul(encoder_embedded[:, -1], W) + b\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
    "        correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_onehot = np.zeros((train_Y.shape[0],len(tag2idx)))\n",
    "train_onehot[np.arange(train_Y.shape[0]),train_Y] = 1.0\n",
    "\n",
    "test_onehot = np.zeros((test_Y.shape[0],len(tag2idx)))\n",
    "test_onehot[np.arange(test_Y.shape[0]),test_Y] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_layer = 128\n",
    "num_layers = 2\n",
    "embedded_size = 128\n",
    "dimension_output = len(tag2idx)\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "idx2tag={idx: tag for tag, idx in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_idx(corpus, dic, UNK=3):\n",
    "    maxlen = max([len(i) for i in corpus])\n",
    "    X = np.zeros((len(corpus),maxlen))\n",
    "    for i in range(len(corpus)):\n",
    "        for no, k in enumerate(corpus[i][:maxlen][::-1]):\n",
    "            try:\n",
    "                X[i,-1 - no]=dic[k]\n",
    "            except Exception as e:\n",
    "                X[i,-1 - no]=UNK\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(size_layer,num_layers,embedded_size,len(char2idx),dimension_output,learning_rate)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.000000, current acc: 0.764556\n",
      "time taken: 9.056696891784668\n",
      "epoch: 0, training loss: 0.973193, training acc: 0.759046, valid loss: 0.868884, valid acc: 0.764556\n",
      "\n",
      "epoch: 1, pass acc: 0.764556, current acc: 0.776563\n",
      "time taken: 8.885531902313232\n",
      "epoch: 1, training loss: 0.835342, training acc: 0.772039, valid loss: 0.763444, valid acc: 0.776563\n",
      "\n",
      "epoch: 2, pass acc: 0.776563, current acc: 0.814803\n",
      "time taken: 8.91784930229187\n",
      "epoch: 2, training loss: 0.731407, training acc: 0.798026, valid loss: 0.657007, valid acc: 0.814803\n",
      "\n",
      "epoch: 3, pass acc: 0.814803, current acc: 0.832319\n",
      "time taken: 8.92201042175293\n",
      "epoch: 3, training loss: 0.647545, training acc: 0.818339, valid loss: 0.585254, valid acc: 0.832319\n",
      "\n",
      "epoch: 4, pass acc: 0.832319, current acc: 0.844737\n",
      "time taken: 8.985709190368652\n",
      "epoch: 4, training loss: 0.585942, training acc: 0.832895, valid loss: 0.534852, valid acc: 0.844737\n",
      "\n",
      "epoch: 5, pass acc: 0.844737, current acc: 0.859704\n",
      "time taken: 8.957971811294556\n",
      "epoch: 5, training loss: 0.538246, training acc: 0.845313, valid loss: 0.494778, valid acc: 0.859704\n",
      "\n",
      "epoch: 6, pass acc: 0.859704, current acc: 0.867681\n",
      "time taken: 8.887683629989624\n",
      "epoch: 6, training loss: 0.496869, training acc: 0.857812, valid loss: 0.456462, valid acc: 0.867681\n",
      "\n",
      "epoch: 7, pass acc: 0.867681, current acc: 0.872697\n",
      "time taken: 8.973226308822632\n",
      "epoch: 7, training loss: 0.461683, training acc: 0.865543, valid loss: 0.426914, valid acc: 0.872697\n",
      "\n",
      "epoch: 8, pass acc: 0.872697, current acc: 0.879934\n",
      "time taken: 8.9468412399292\n",
      "epoch: 8, training loss: 0.431054, training acc: 0.871711, valid loss: 0.398486, valid acc: 0.879934\n",
      "\n",
      "epoch: 9, pass acc: 0.879934, current acc: 0.886431\n",
      "time taken: 8.931610345840454\n",
      "epoch: 9, training loss: 0.403895, training acc: 0.876398, valid loss: 0.374983, valid acc: 0.886431\n",
      "\n",
      "epoch: 10, pass acc: 0.886431, current acc: 0.887911\n",
      "time taken: 8.90331768989563\n",
      "epoch: 10, training loss: 0.380697, training acc: 0.880510, valid loss: 0.354800, valid acc: 0.887911\n",
      "\n",
      "epoch: 11, pass acc: 0.887911, current acc: 0.892270\n",
      "time taken: 8.885267972946167\n",
      "epoch: 11, training loss: 0.360132, training acc: 0.884704, valid loss: 0.337609, valid acc: 0.892270\n",
      "\n",
      "epoch: 12, pass acc: 0.892270, current acc: 0.895230\n",
      "time taken: 8.900665760040283\n",
      "epoch: 12, training loss: 0.342126, training acc: 0.889638, valid loss: 0.323892, valid acc: 0.895230\n",
      "\n",
      "epoch: 13, pass acc: 0.895230, current acc: 0.896711\n",
      "time taken: 8.938123941421509\n",
      "epoch: 13, training loss: 0.326677, training acc: 0.892928, valid loss: 0.311103, valid acc: 0.896711\n",
      "\n",
      "epoch: 14, pass acc: 0.896711, current acc: 0.898684\n",
      "time taken: 8.929168939590454\n",
      "epoch: 14, training loss: 0.313111, training acc: 0.894819, valid loss: 0.301904, valid acc: 0.898684\n",
      "\n",
      "epoch: 15, pass acc: 0.898684, current acc: 0.900822\n",
      "time taken: 8.925221920013428\n",
      "epoch: 15, training loss: 0.304412, training acc: 0.896299, valid loss: 0.292241, valid acc: 0.900822\n",
      "\n",
      "epoch: 16, pass acc: 0.900822, current acc: 0.901316\n",
      "time taken: 8.890416145324707\n",
      "epoch: 16, training loss: 0.292441, training acc: 0.900082, valid loss: 0.286087, valid acc: 0.901316\n",
      "\n",
      "epoch: 17, pass acc: 0.901316, current acc: 0.903372\n",
      "time taken: 8.871411085128784\n",
      "epoch: 17, training loss: 0.284136, training acc: 0.899260, valid loss: 0.278656, valid acc: 0.903372\n",
      "\n",
      "epoch: 18, pass acc: 0.903372, current acc: 0.903783\n",
      "time taken: 8.957414150238037\n",
      "epoch: 18, training loss: 0.274432, training acc: 0.902385, valid loss: 0.273690, valid acc: 0.903783\n",
      "\n",
      "epoch: 19, pass acc: 0.903783, current acc: 0.906086\n",
      "time taken: 8.927985668182373\n",
      "epoch: 19, training loss: 0.267919, training acc: 0.904276, valid loss: 0.267185, valid acc: 0.906086\n",
      "\n",
      "epoch: 20, pass acc: 0.906086, current acc: 0.906743\n",
      "time taken: 8.935180425643921\n",
      "epoch: 20, training loss: 0.261881, training acc: 0.905263, valid loss: 0.260859, valid acc: 0.906743\n",
      "\n",
      "epoch: 21, pass acc: 0.906743, current acc: 0.907730\n",
      "time taken: 8.885449171066284\n",
      "epoch: 21, training loss: 0.255143, training acc: 0.906086, valid loss: 0.255797, valid acc: 0.907730\n",
      "\n",
      "epoch: 22, pass acc: 0.907730, current acc: 0.908470\n",
      "time taken: 8.903314590454102\n",
      "epoch: 22, training loss: 0.249866, training acc: 0.907484, valid loss: 0.250710, valid acc: 0.908470\n",
      "\n",
      "epoch: 23, pass acc: 0.908470, current acc: 0.909868\n",
      "time taken: 8.933691263198853\n",
      "epoch: 23, training loss: 0.245582, training acc: 0.907813, valid loss: 0.244619, valid acc: 0.909868\n",
      "\n",
      "epoch: 24, pass acc: 0.909868, current acc: 0.910937\n",
      "time taken: 8.86955189704895\n",
      "epoch: 24, training loss: 0.241464, training acc: 0.909704, valid loss: 0.240644, valid acc: 0.910937\n",
      "\n",
      "epoch: 25, pass acc: 0.910937, current acc: 0.912336\n",
      "time taken: 8.83117127418518\n",
      "epoch: 25, training loss: 0.237555, training acc: 0.908882, valid loss: 0.237020, valid acc: 0.912336\n",
      "\n",
      "epoch: 26, pass acc: 0.912336, current acc: 0.912500\n",
      "time taken: 8.919925928115845\n",
      "epoch: 26, training loss: 0.234180, training acc: 0.910362, valid loss: 0.234773, valid acc: 0.912500\n",
      "\n",
      "time taken: 8.888478994369507\n",
      "epoch: 27, training loss: 0.231740, training acc: 0.910197, valid loss: 0.234847, valid acc: 0.912336\n",
      "\n",
      "time taken: 8.88985800743103\n",
      "epoch: 28, training loss: 0.230048, training acc: 0.910526, valid loss: 0.235158, valid acc: 0.911102\n",
      "\n",
      "time taken: 8.866965293884277\n",
      "epoch: 29, training loss: 0.227097, training acc: 0.910609, valid loss: 0.234649, valid acc: 0.909951\n",
      "\n",
      "time taken: 8.917338848114014\n",
      "epoch: 30, training loss: 0.225319, training acc: 0.911184, valid loss: 0.238475, valid acc: 0.908553\n",
      "\n",
      "time taken: 8.836161136627197\n",
      "epoch: 31, training loss: 0.227009, training acc: 0.908717, valid loss: 0.237118, valid acc: 0.909704\n",
      "\n",
      "break epoch:32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 5, 0, 0, 0\n",
    "while True:\n",
    "    lasttime = time.time()\n",
    "    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n",
    "        print('break epoch:%d\\n'%(EPOCH))\n",
    "        break\n",
    "        \n",
    "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
    "    for i in range(0, (len(train_X) // batch_size) * batch_size, batch_size):\n",
    "        batch_x = str_idx(train_words[i:i+batch_size],char2idx)\n",
    "        acc, loss, _ = sess.run([model.accuracy, model.cost, model.optimizer], \n",
    "                           feed_dict = {model.X : batch_x, model.Y : train_onehot[i:i+batch_size]})\n",
    "        train_loss += loss\n",
    "        train_acc += acc\n",
    "    \n",
    "    for i in range(0, (len(test_X) // batch_size) * batch_size, batch_size):\n",
    "        batch_x = str_idx(test_words[i:i+batch_size],char2idx)\n",
    "        acc, loss = sess.run([model.accuracy, model.cost], \n",
    "                           feed_dict = {model.X : batch_x, model.Y : test_onehot[i:i+batch_size]})\n",
    "        test_loss += loss\n",
    "        test_acc += acc\n",
    "    \n",
    "    train_loss /= (len(train_X) // batch_size)\n",
    "    train_acc /= (len(train_X) // batch_size)\n",
    "    test_loss /= (len(test_X) // batch_size)\n",
    "    test_acc /= (len(test_X) // batch_size)\n",
    "    \n",
    "    if test_acc > CURRENT_ACC:\n",
    "        print('epoch: %d, pass acc: %f, current acc: %f'%(EPOCH,CURRENT_ACC, test_acc))\n",
    "        CURRENT_ACC = test_acc\n",
    "        CURRENT_CHECKPOINT = 0\n",
    "    else:\n",
    "        CURRENT_CHECKPOINT += 1\n",
    "        \n",
    "    print('time taken:', time.time()-lasttime)\n",
    "    print('epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'%(EPOCH,train_loss,\n",
    "                                                                                          train_acc,test_loss,\n",
    "                                                                                          test_acc))\n",
    "    EPOCH += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity(string):\n",
    "    batch_x = str_idx([process_word(w) for w in string.split()],char2idx)\n",
    "    Y_pred = sess.run(model.logits,feed_dict={model.X:batch_x})\n",
    "    for no, i in enumerate(string.split()):\n",
    "        print(i,idx2tag[np.argmax(Y_pred[no])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KUALA LOC\n",
      "LUMPUR: LOC\n",
      "Sempena O\n",
      "sambutan O\n",
      "Aidilfitri EVENT\n",
      "minggu O\n",
      "depan, O\n",
      "Perdana PRN\n",
      "Menteri PRN\n",
      "Tun PRN\n",
      "Dr PRN\n",
      "Mahathir PRN\n",
      "Mohamad PRN\n",
      "dan O\n",
      "Menteri PRN\n",
      "Pengangkutan NORP\n",
      "Anthony PRN\n",
      "Loke PRN\n",
      "Siew PRN\n",
      "Fook O\n",
      "menitipkan O\n",
      "pesanan O\n",
      "khas NORP\n",
      "kepada O\n",
      "orang O\n",
      "ramai O\n",
      "yang O\n",
      "mahu O\n",
      "pulang O\n",
      "ke O\n",
      "kampung LOC\n",
      "halaman O\n",
      "masing-masing. O\n",
      "Dalam O\n",
      "video O\n",
      "pendek O\n",
      "terbitan O\n",
      "Jabatan NORP\n",
      "Keselamatan O\n",
      "Jalan LOC\n",
      "Raya O\n",
      "(JKJR) O\n",
      "itu, O\n",
      "Dr PRN\n",
      "Mahathir PRN\n",
      "menasihati O\n",
      "mereka O\n",
      "supaya O\n",
      "berhenti O\n",
      "berehat O\n",
      "dan O\n",
      "tidur O\n",
      "sebentar O\n",
      "sekiranya O\n",
      "mengantuk O\n",
      "ketika O\n",
      "memandu. O\n"
     ]
    }
   ],
   "source": [
    "get_entity('KUALA LUMPUR: Sempena sambutan Aidilfitri minggu depan, Perdana Menteri Tun Dr Mahathir Mohamad dan Menteri Pengangkutan Anthony Loke Siew Fook menitipkan pesanan khas kepada orang ramai yang mahu pulang ke kampung halaman masing-masing. Dalam video pendek terbitan Jabatan Keselamatan Jalan Raya (JKJR) itu, Dr Mahathir menasihati mereka supaya berhenti berehat dan tidur sebentar  sekiranya mengantuk ketika memandu.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
