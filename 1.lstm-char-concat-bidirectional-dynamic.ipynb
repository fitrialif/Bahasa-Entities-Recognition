{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12194"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('entities-bm.txt','r') as fopen:\n",
    "    texts= list(filter(None, fopen.read().split('\\n')))\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = texts\n",
    "# dataset is too small\n",
    "test_texts = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {'PAD': 0,'NUM':1,'UNK':2}\n",
    "tag2idx = {'PAD': 0}\n",
    "char2idx = {'PAD': 0}\n",
    "word_idx = 3\n",
    "tag_idx = 1\n",
    "char_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(word, lower=True):\n",
    "    if lower:\n",
    "        word = word.lower()\n",
    "    else:\n",
    "        if word.isupper():\n",
    "            word = word.title()\n",
    "    word = re.sub('[^A-Za-z0-9\\- ]+', '', word)\n",
    "    if word.isdigit():\n",
    "        word = 'NUM'\n",
    "    return word\n",
    "\n",
    "def read_file(f):\n",
    "    global word_idx, tag_idx, char_idx\n",
    "    words, tags, X, Y = [], [], [], []\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if (len(line) == 0 or line.startswith(\"-DOCSTART-\")):\n",
    "            continue\n",
    "        else:\n",
    "            ls = line.split(' ')\n",
    "            if len(ls) > 1:\n",
    "                word, tag = ls[0],ls[-1]\n",
    "            else:\n",
    "                word = ls[0]\n",
    "                tag = 'O'\n",
    "            for c in word:\n",
    "                if c not in char2idx:\n",
    "                    char2idx[c] = char_idx\n",
    "                    char_idx += 1\n",
    "            word = process_word(word)\n",
    "            if len(word) < 1:\n",
    "                continue\n",
    "            words += [word]\n",
    "            tags += [tag]\n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = word_idx\n",
    "                word_idx += 1\n",
    "            X.append(word2idx[word])\n",
    "            if tag not in tag2idx:\n",
    "                tag2idx[tag] = tag_idx\n",
    "                tag_idx += 1\n",
    "            Y.append(tag2idx[tag])\n",
    "                        \n",
    "    return words, tags, X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words, train_tags, train_X, train_Y = read_file(train_texts)\n",
    "test_words, test_tags, test_X, test_Y = read_file(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2tag={idx: tag for tag, idx in tag2idx.items()}\n",
    "idx2word={idx: tag for tag, idx in word2idx.items()}\n",
    "batch_size = 16\n",
    "dim_word = 128\n",
    "dim_char = 32\n",
    "dropout = 0.8\n",
    "learning_rate = 1e-2\n",
    "hidden_size_char = 64\n",
    "hidden_size_word = 128\n",
    "num_layers = 2\n",
    "seq_len = 20\n",
    "display_step = 200\n",
    "epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, dim_word, dim_char, dropout, learning_rate,\n",
    "                 hidden_size_char, hidden_size_word, num_layers):\n",
    "        \n",
    "        def cells(size, reuse=False):\n",
    "            return tf.nn.rnn_cell.LSTMCell(size,initializer=tf.orthogonal_initializer(),reuse=reuse)\n",
    "        \n",
    "        def clip_grads(loss):\n",
    "            variables = tf.trainable_variables()\n",
    "            grads = tf.gradients(loss, variables)\n",
    "            clipped_grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "            return zip(clipped_grads, variables)\n",
    "        \n",
    "        self.word_ids = tf.placeholder(tf.int32, shape=[None, None])\n",
    "        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None])\n",
    "        self.char_ids = tf.placeholder(tf.int32, shape=[None, None, None])\n",
    "        self.word_lengths = tf.placeholder(tf.int32, shape=[None, None])\n",
    "        self.labels = tf.placeholder(tf.int32, shape=[None, None])\n",
    "        \n",
    "        self.word_embeddings = tf.Variable(tf.truncated_normal([len(word2idx), dim_word],\n",
    "                                                      stddev=1.0 / np.sqrt(dim_word)))\n",
    "        self.char_embeddings = tf.Variable(tf.truncated_normal([len(char2idx), dim_char],\n",
    "                                                      stddev=1.0 / np.sqrt(dim_char)))\n",
    "        word_embedded = tf.nn.embedding_lookup(self.word_embeddings, self.word_ids)\n",
    "        char_embedded = tf.nn.embedding_lookup(self.char_embeddings, self.char_ids)\n",
    "        s = tf.shape(char_embedded)\n",
    "        char_embedded = tf.reshape(char_embedded, shape=[s[0]*s[1], s[-2], dim_char])\n",
    "        word_lengths = tf.reshape(self.word_lengths, shape=[s[0]*s[1]])\n",
    "        for n in range(num_layers):\n",
    "            (out_fw, out_bw), (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = cells(hidden_size_char),\n",
    "                cell_bw = cells(hidden_size_char),\n",
    "                inputs = char_embedded,\n",
    "                dtype = tf.float32,\n",
    "                sequence_length=word_lengths,\n",
    "                scope = 'bidirectional_rnn_char_%d'%(n))\n",
    "            char_embedded = tf.concat((out_fw, out_bw), 2)\n",
    "        output = tf.reshape(char_embedded[:,-1], shape=[s[0], s[1], 2*hidden_size_char])\n",
    "        word_embedded = tf.concat([word_embedded, output], axis=-1)\n",
    "        word_embedded = tf.nn.dropout(word_embedded, dropout)\n",
    "        \n",
    "        for n in range(num_layers):\n",
    "            (out_fw, out_bw), (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = cells(hidden_size_word),\n",
    "                cell_bw = cells(hidden_size_word),\n",
    "                inputs = word_embedded,\n",
    "                sequence_length=self.sequence_lengths, \n",
    "                dtype=tf.float32,\n",
    "                scope = 'bidirectional_rnn_word_%d'%(n))\n",
    "            word_embedded = tf.concat((out_fw, out_bw), 2)\n",
    "        word_embedded = tf.nn.dropout(word_embedded, dropout)\n",
    "        \n",
    "        W = tf.get_variable('w',shape=(2*hidden_size_word, len(idx2tag)),\n",
    "                            initializer=tf.orthogonal_initializer())\n",
    "        b = tf.get_variable('b',shape=(len(idx2tag)),initializer=tf.zeros_initializer())\n",
    "        \n",
    "        nsteps = tf.shape(word_embedded)[1]\n",
    "        output = tf.reshape(word_embedded, [-1, 2*hidden_size_word])\n",
    "        pred = tf.matmul(output, W) + b\n",
    "        self.logits = tf.reshape(pred, [-1, nsteps, len(idx2tag)])\n",
    "        \n",
    "        log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(\n",
    "        self.logits, self.labels, tf.count_nonzero(self.word_ids, 1))\n",
    "        \n",
    "        self.cost = tf.reduce_mean(-log_likelihood)\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "        self.crf_decode = tf.contrib.crf.crf_decode(self.logits, \n",
    "                                                    trans_params, \n",
    "                                                    tf.count_nonzero(self.word_ids, 1))[0]\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).apply_gradients(clip_grads(self.cost), \n",
    "                                                                                    global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(dim_word,dim_char,dropout,learning_rate,hidden_size_char,hidden_size_word,num_layers)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_seq(x):\n",
    "    return np.array([x[i: i+seq_len] for i in range(0, len(x)-seq_len, 1)])\n",
    "\n",
    "def to_train_seq(*args):\n",
    "    return [iter_seq(x) for x in args]\n",
    "\n",
    "def generate_char_seq(batch):\n",
    "    x = [[len(idx2word[i]) for i in k] for k in batch]\n",
    "    maxlen = max([j for i in x for j in i])\n",
    "    temp = np.zeros((batch.shape[0],batch.shape[1],maxlen),dtype=np.int32)\n",
    "    for i in range(batch.shape[0]):\n",
    "        for k in range(batch.shape[1]):\n",
    "            for no, c in enumerate(idx2word[batch[i,k]]):\n",
    "                temp[i,k,no] = char2idx[c]\n",
    "    return temp, np.array(x,dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_seq, train_Y_seq = to_train_seq(train_X, train_Y)\n",
    "test_X_seq, test_Y_seq = to_train_seq(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]),\n",
       " array([  8063, 185166,  23771,   2600,   6080,   2120,   2120,  10280,\n",
       "          1280,   1860,     20]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_Y_seq.ravel(),return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]),\n",
       " array([  8063, 185166,  23771,   2600,   6080,   2120,   2120,  10280,\n",
       "          1280,   1860,     20]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test_Y_seq.ravel(),return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'PAD',\n",
       " 1: 'LOC',\n",
       " 2: 'O',\n",
       " 3: 'PRN',\n",
       " 4: 'ORG',\n",
       " 5: 'TIME',\n",
       " 6: 'ART',\n",
       " 7: 'EVENT',\n",
       " 8: 'NORP',\n",
       " 9: 'FAC',\n",
       " 10: 'LAW',\n",
       " 11: 'DOC'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 1, loss 55.154758\n",
      "epoch 1, step 200, loss 2.997496\n",
      "epoch 1, step 400, loss 2.384757\n",
      "epoch 1, step 600, loss 10.292503\n",
      "epoch 1, avg loss 11.713000\n",
      "epoch 2, step 800, loss 5.311782\n",
      "epoch 2, step 1000, loss 17.896744\n",
      "epoch 2, step 1200, loss 14.790212\n",
      "epoch 2, step 1400, loss 8.898263\n",
      "epoch 2, avg loss 6.322279\n",
      "epoch 3, step 1600, loss 0.008326\n",
      "epoch 3, step 1800, loss 2.825358\n",
      "epoch 3, step 2000, loss 1.019447\n",
      "epoch 3, step 2200, loss 2.831027\n",
      "epoch 3, avg loss 4.785942\n",
      "epoch 4, step 2400, loss 0.042604\n",
      "epoch 4, step 2600, loss 1.630599\n",
      "epoch 4, step 2800, loss 0.235781\n",
      "epoch 4, step 3000, loss 11.383674\n",
      "epoch 4, avg loss 3.816747\n",
      "epoch 5, step 3200, loss 1.631361\n",
      "epoch 5, step 3400, loss 11.791616\n",
      "epoch 5, step 3600, loss 8.646818\n",
      "epoch 5, step 3800, loss 0.330976\n",
      "epoch 5, avg loss 3.119967\n",
      "epoch 6, step 4000, loss 0.054771\n",
      "epoch 6, step 4200, loss 1.198393\n",
      "epoch 6, step 4400, loss 1.082042\n",
      "epoch 6, avg loss 2.603878\n",
      "epoch 7, step 4600, loss 0.303289\n",
      "epoch 7, step 4800, loss 11.927292\n",
      "epoch 7, step 5000, loss 3.721798\n",
      "epoch 7, step 5200, loss 1.856673\n",
      "epoch 7, avg loss 2.137955\n",
      "epoch 8, step 5400, loss 0.000526\n",
      "epoch 8, step 5600, loss 1.597943\n",
      "epoch 8, step 5800, loss 0.758090\n",
      "epoch 8, step 6000, loss 0.551716\n",
      "epoch 8, avg loss 1.774423\n",
      "epoch 9, step 6200, loss 0.198771\n",
      "epoch 9, step 6400, loss 1.078884\n",
      "epoch 9, step 6600, loss 0.369281\n",
      "epoch 9, step 6800, loss 4.010623\n",
      "epoch 9, avg loss 1.444157\n",
      "epoch 10, step 7000, loss 0.205262\n",
      "epoch 10, step 7200, loss 8.397850\n",
      "epoch 10, step 7400, loss 4.050125\n",
      "epoch 10, step 7600, loss 0.222540\n",
      "epoch 10, avg loss 1.181091\n",
      "epoch 11, step 7800, loss 0.010678\n",
      "epoch 11, step 8000, loss 0.822674\n",
      "epoch 11, step 8200, loss 0.416552\n",
      "epoch 11, avg loss 0.993390\n",
      "epoch 12, step 8400, loss 0.115506\n",
      "epoch 12, step 8600, loss 3.145156\n",
      "epoch 12, step 8800, loss 0.419340\n",
      "epoch 12, step 9000, loss 0.304397\n",
      "epoch 12, avg loss 0.830656\n",
      "epoch 13, step 9200, loss 0.000865\n",
      "epoch 13, step 9400, loss 0.613782\n",
      "epoch 13, step 9600, loss 0.119336\n",
      "epoch 13, step 9800, loss 1.247524\n",
      "epoch 13, avg loss 0.771386\n",
      "epoch 14, step 10000, loss 0.008772\n",
      "epoch 14, step 10200, loss 0.931631\n",
      "epoch 14, step 10400, loss 0.228822\n",
      "epoch 14, step 10600, loss 1.501752\n",
      "epoch 14, avg loss 0.748192\n",
      "epoch 15, step 10800, loss 0.250011\n",
      "epoch 15, step 11000, loss 0.536016\n",
      "epoch 15, step 11200, loss 0.913229\n",
      "epoch 15, step 11400, loss 0.024961\n",
      "epoch 15, avg loss 0.682044\n",
      "epoch 16, step 11600, loss 0.024876\n",
      "epoch 16, step 11800, loss 0.575319\n",
      "epoch 16, step 12000, loss 0.235305\n",
      "epoch 16, avg loss 0.690857\n",
      "epoch 17, step 12200, loss 0.139123\n",
      "epoch 17, step 12400, loss 3.192908\n",
      "epoch 17, step 12600, loss 0.751548\n",
      "epoch 17, step 12800, loss 0.196697\n",
      "epoch 17, avg loss 0.666997\n",
      "epoch 18, step 13000, loss 0.045638\n",
      "epoch 18, step 13200, loss 0.154280\n",
      "epoch 18, step 13400, loss 0.368721\n",
      "epoch 18, step 13600, loss 0.829372\n",
      "epoch 18, avg loss 0.560339\n",
      "epoch 19, step 13800, loss 0.015087\n",
      "epoch 19, step 14000, loss 0.601974\n",
      "epoch 19, step 14200, loss 0.027675\n",
      "epoch 19, step 14400, loss 2.714871\n",
      "epoch 19, avg loss 0.581807\n",
      "epoch 20, step 14600, loss 0.096676\n",
      "epoch 20, step 14800, loss 1.074320\n",
      "epoch 20, step 15000, loss 0.992944\n",
      "epoch 20, step 15200, loss 0.057637\n",
      "epoch 20, avg loss 0.566718\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    total_cost = 0\n",
    "    for k in range(0,(train_X_seq.shape[0] // batch_size)*batch_size,batch_size):\n",
    "        batch_x = train_X_seq[k:k+batch_size]\n",
    "        batch_y = train_Y_seq[k:k+batch_size]\n",
    "        batch_length = [seq_len] * batch_size\n",
    "        batch_x_char, batch_x_char_length = generate_char_seq(batch_x)\n",
    "        step, loss, _ = sess.run([model.global_step, model.cost, model.optimizer],\n",
    "                                 feed_dict={model.word_ids:batch_x,\n",
    "                                           model.sequence_lengths:batch_length,\n",
    "                                           model.char_ids:batch_x_char,\n",
    "                                           model.word_lengths:batch_x_char_length,\n",
    "                                           model.labels:batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            print('epoch %d, step %d, loss %f'%(i+1,step,loss))\n",
    "        total_cost += loss\n",
    "    total_cost /= (train_X_seq.shape[0] // batch_size)\n",
    "    print('epoch %d, avg loss %f'%(i+1,total_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_Y, predicted_Y = [], []\n",
    "for k in range(0,(test_X_seq.shape[0] // batch_size)*batch_size,batch_size):\n",
    "    batch_x = test_X_seq[k:k+batch_size]\n",
    "    batch_length = [seq_len] * batch_size\n",
    "    batch_x_char, batch_x_char_length = generate_char_seq(batch_x)\n",
    "    batch_y = test_Y_seq[k:k+batch_size]\n",
    "    Y_pred = sess.run(model.crf_decode,\n",
    "                  feed_dict={model.word_ids:batch_x,\n",
    "                             model.sequence_lengths:batch_length,\n",
    "                            model.char_ids:batch_x_char,\n",
    "                            model.word_lengths:batch_x_char_length})\n",
    "    predicted_Y.append(Y_pred)\n",
    "    label_Y.append(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        PAD       0.98      0.98      0.98      8063\n",
      "        FAC       0.99      1.00      0.99    185006\n",
      "        ORG       0.99      0.95      0.97     23771\n",
      "      EVENT       0.97      0.99      0.98      2600\n",
      "        PRN       0.96      0.91      0.93      6080\n",
      "        LOC       0.98      0.95      0.97      2120\n",
      "          O       0.96      0.92      0.94      2120\n",
      "        LAW       0.98      0.93      0.95     10280\n",
      "       NORP       0.96      0.99      0.97      1280\n",
      "       TIME       0.99      0.94      0.97      1860\n",
      "        ART       0.59      0.50      0.54        20\n",
      "\n",
      "avg / total       0.99      0.99      0.99    243200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 11, does not match size of target_names, 12\n",
      "  .format(len(labels), len(target_names))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.vstack(label_Y).ravel(), np.vstack(predicted_Y).ravel(), target_names=tag2idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = 'Keikhlasan merupakan faktor utama yang perlu ada pada setiap pemimpin dan ahli UMNO sekiranya mahu melihat parti itu pulih kembali selain mendapat sokongan majoriti rakyat negara ini, kata Ahli Majlis Tertinggi (MT) UMNO Datuk Seri Idris Jusoh.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = []\n",
    "for w in test_string.split():\n",
    "    w = process_word(w)\n",
    "    try:\n",
    "        test_X.append(word2idx[w])\n",
    "    except:\n",
    "        test_X.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x_char, batch_x_char_length = generate_char_seq(np.array([test_X]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = sess.run(model.crf_decode,feed_dict={model.word_ids:np.array([test_X]),\n",
    "                                              model.char_ids:batch_x_char,\n",
    "                                             model.sequence_lengths:[seq_len],\n",
    "                                             model.word_lengths:batch_x_char_length})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keikhlasan O\n",
      "merupakan O\n",
      "faktor O\n",
      "utama O\n",
      "yang O\n",
      "perlu O\n",
      "ada O\n",
      "pada O\n",
      "setiap O\n",
      "pemimpin O\n",
      "dan O\n",
      "ahli O\n",
      "UMNO NORP\n",
      "sekiranya O\n",
      "mahu O\n",
      "melihat O\n",
      "parti O\n",
      "itu O\n",
      "pulih O\n",
      "kembali O\n",
      "selain FAC\n",
      "mendapat FAC\n",
      "sokongan FAC\n",
      "majoriti FAC\n",
      "rakyat FAC\n",
      "negara FAC\n",
      "ini, FAC\n",
      "kata FAC\n",
      "Ahli FAC\n",
      "Majlis FAC\n",
      "Tertinggi FAC\n",
      "(MT) FAC\n",
      "UMNO FAC\n",
      "Datuk FAC\n",
      "Seri FAC\n",
      "Idris FAC\n",
      "Jusoh. FAC\n"
     ]
    }
   ],
   "source": [
    "for no, i in enumerate(test_string.split()):\n",
    "    print(i,idx2tag[Y_pred[0,no]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity(string):\n",
    "    test_X = []\n",
    "    for w in string.split():\n",
    "        w = process_word(w)\n",
    "        try:\n",
    "            test_X.append(word2idx[w])\n",
    "        except:\n",
    "            test_X.append(2)\n",
    "    batch_x_char, batch_x_char_length = generate_char_seq(np.array([test_X]))\n",
    "    Y_pred = sess.run(model.crf_decode,feed_dict={model.word_ids:np.array([test_X]),\n",
    "                                              model.char_ids:batch_x_char,\n",
    "                                             model.sequence_lengths:[seq_len],\n",
    "                                             model.word_lengths:batch_x_char_length})\n",
    "    for no, i in enumerate(string.split()):\n",
    "        print(i,idx2tag[Y_pred[0,no]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KUALA LOC\n",
      "LUMPUR: LOC\n",
      "Sempena O\n",
      "sambutan O\n",
      "Aidilfitri O\n",
      "minggu TIME\n",
      "depan, TIME\n",
      "Perdana PRN\n",
      "Menteri PRN\n",
      "Tun PRN\n",
      "Dr PRN\n",
      "Mahathir PRN\n",
      "Mohamad PRN\n",
      "dan O\n",
      "Menteri PRN\n",
      "Pengangkutan PRN\n",
      "Anthony PRN\n",
      "Loke PRN\n",
      "Siew PRN\n",
      "Fook PRN\n",
      "menitipkan EVENT\n",
      "pesanan EVENT\n",
      "khas EVENT\n",
      "kepada EVENT\n",
      "orang EVENT\n",
      "ramai EVENT\n",
      "yang EVENT\n",
      "mahu EVENT\n",
      "pulang EVENT\n",
      "ke EVENT\n",
      "kampung EVENT\n",
      "halaman EVENT\n",
      "masing-masing. EVENT\n",
      "Dalam EVENT\n",
      "video EVENT\n",
      "pendek EVENT\n",
      "terbitan EVENT\n",
      "Jabatan EVENT\n",
      "Keselamatan EVENT\n",
      "Jalan EVENT\n",
      "Raya EVENT\n",
      "(JKJR) EVENT\n",
      "itu, EVENT\n",
      "Dr EVENT\n",
      "Mahathir EVENT\n",
      "menasihati EVENT\n",
      "mereka EVENT\n",
      "supaya EVENT\n",
      "berhenti EVENT\n",
      "berehat EVENT\n",
      "dan EVENT\n",
      "tidur EVENT\n",
      "sebentar EVENT\n",
      "sekiranya EVENT\n",
      "mengantuk EVENT\n",
      "ketika EVENT\n",
      "memandu. EVENT\n"
     ]
    }
   ],
   "source": [
    "get_entity('KUALA LUMPUR: Sempena sambutan Aidilfitri minggu depan, Perdana Menteri Tun Dr Mahathir Mohamad dan Menteri Pengangkutan Anthony Loke Siew Fook menitipkan pesanan khas kepada orang ramai yang mahu pulang ke kampung halaman masing-masing. Dalam video pendek terbitan Jabatan Keselamatan Jalan Raya (JKJR) itu, Dr Mahathir menasihati mereka supaya berhenti berehat dan tidur sebentar  sekiranya mengantuk ketika memandu.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
